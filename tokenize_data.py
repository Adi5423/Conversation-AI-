import json
from transformers import AutoTokenizer

location = "Dataset//pre_process//Final_json//"

# Load the JSON file
with open(location+"Arohii.json", "r",encoding = "utf-8") as input_file:
    data = json.load(input_file)

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/xtremedistil-l6-h256-uncased")
tokenizer.add_special_tokens({'additional_special_tokens': ['ğŸ˜Š', 'ğŸ«¶ğŸ»','ğŸ«€','ğŸ¤ŒğŸ»', 'ğŸ¤”','ğŸ˜‚','ğŸ¥¹','ğŸ« ','ğŸ˜­','ğŸ’Œ','ğŸ«‚','ğŸ¥±','ğŸ˜´','ğŸ’“','ğŸ’','ğŸ’–','ğŸ’','ğŸ’˜','â¤','â™¥','ğŸ’«']})

# Tokenize the responses
for item in data:
    response = item["response"]
    tokens = tokenizer.tokenize(response)
    item["tokens"] = tokens

# Save the tokenized data to a new JSON file
with open("Dataset//pre_process//Final_json//tokenization//tokenized_Arohii.json", "w",encoding = "utf-8") as output_file:
    json.dump(data, output_file, indent=2)